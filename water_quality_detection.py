# -*- coding: utf-8 -*-
"""Water Quality detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CsON4GMega2Y0i2N1Kv8XaE619gdeh5o

# **Mount Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.metrics import  accuracy_score, classification_report , confusion_matrix

"""# **Read Dataset: .csv/.xlsx to DataFrame**"""

dataset = pd.read_excel('/content/drive/MyDrive/water_quality - Sheet1.xlsx')
#dataset =pd.read_excel("/content/water_quality - Sheet1.xlsx")
#dataset = pd.read_excel('/content/water_quality - Sheet1 (1).xlsx')
#dataset = pd.read_excel('/content/cse422_water_quality - Sheet1.xlsx')
dataset.head(5)

"""# **Dataset Analysis**

- shape
- columns
- head()
- sample()
- describe()
- finding categorical features
- checking if any null/nan values
- filtering data instances based on indices
- filtering data instances based on conditional statements

"""

dataset.shape

display(dataset.describe().T)

dataset.describe()

dataset.columns



dataset.info()

#dataset.head(10)
dataset.head()

#dataset.sample(5)
dataset.sample()

#Finding the unique values
dataset['is_safe'].unique()

dataset['is_safe2'] = dataset['is_safe'].map({'Yes': 1,'No':0}) #finding categorical feature and mapping them

dataset[["is_safe", "is_safe2"]]

dataset.isnull().sum()

"""# **Dataset Visualization**

- Dataset biased or not
- Histogram
- Scatter Plot
- Bar chart
- etc.
"""

fig = px.scatter(dataset,x="bacteria", y="arsenic",color= "is_safe")
fig.show()

dataset['is_safe'].hist(bins=20)

sns.barplot(dataset)
plt.show()

"""# **Dataset Pre-processing**

- Null/NaN values handling

    a) Delete Rows

    b) Delete Columns

    c) Impute Values

- Encoding Categorical Features

- Feature Scaling
"""

dataset.isnull().sum()

#rmv column with biggest null values
dataset = dataset.drop(['phosphate'], axis = 1)

dataset.isnull().sum()

#imputing null values
dataset.isnull().sum().sum()

"""# Imputation"""

from sklearn.impute import SimpleImputer
import numpy as np

# The imputation strategy:  most_frequent

impute = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

list_of_features = ["magnesium"]

dataset[list_of_features] = impute.fit_transform(dataset[list_of_features])

dataset.isnull().sum()

"""# Label Encoding"""

from sklearn.preprocessing import LabelEncoder  ####encoding

# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
dataset['is_safe2'] = enc.fit_transform(dataset['is_safe'])

# Compare the two columns
print(dataset[['is_safe', 'is_safe2']].head())

dataset.head()

dataset = dataset.drop(["is_safe"], axis=1) ###dropping yes/no keeping 0/1

dataset.head() ##checking dropping task

## Feature scalling - booliean values convertion
x = dataset.drop("is_safe2", axis=1) ##feature
y = dataset["is_safe2"]   ###boolean value

plt.show()

x

y

x.shape , y.shape

from sklearn.preprocessing import StandardScaler ###feature scaling

scaler = StandardScaler()

x = scaler.fit_transform(x)

dataset.head()

"""# **Feature Selection**"""

dataset_corr = dataset.corr()

import seaborn as sns #correlation

sns.heatmap(dataset_corr, cmap = 'rocket_r')

sns.heatmap(dataset_corr, cmap ="YlGnBu")

dataset = dataset.select_dtypes(include = [np.number])
sns.heatmap(dataset, cmap = 'YlGnBu')
plt.show()

"""# **Dataset Splitting**

- Training Set

- Testing Set
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder, StandardScaler



dataset.fillna(dataset.mean(), inplace=True)

# Separate features (X) and target variable (y)
X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]

# Label encode the target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)


X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,stratify = y)
#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state=0)

len(X_train)

len(y_train)

len(X_test)

len(y_test)

"""# **Dataset Training**

Classification Problem

- SVM

- Linear Regression

- Decision Tree

- K-nearest Neighbors

- Logistic Regression

# SVM
"""

#########################
#########SVM#############
########################

from sklearn.svm import SVC
#creating obj of model
model_svm = SVC(kernel='rbf')

#model training svm
model_svm.fit(X_train,y_train)

#prediction
pred_svm = model_svm.predict(X_test)

accuracy_score_svm = accuracy_score(y_test,pred_svm) #svm accuracy score
print("Accuracy: ",accuracy_score_svm*100)

print("confusion_matrix: \n", confusion_matrix(y_test, pred_svm))

"""# Linear Regression

"""

# Remove rows with NaN values
dataset.dropna(inplace=True)

# Fill NaN values with the mean of each column
dataset.fillna(dataset.mean(), inplace=True)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder, StandardScaler



dataset.fillna(dataset.mean(), inplace=True)

# Separate features (X) and target variable (y)
X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]

# Label encode the target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)


X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


model = LinearRegression()

# Train
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")

#print("confusion_matrix: \n", confusion_matrix(y_test, y_pred))
#print(classification_report(y_test, y_pred))

"""# Decision Tree"""

from sklearn.model_selection import train_test_split
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier

# Load the dataset
# iris = load_iris()
X = dataset.iloc[:,:-1]
y = dataset.iloc[:,-1]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a HistGradientBoostingClassifier
# clf = HistGradientBoostingClassifier(random_state=100, max_depth=10, min_samples_leaf=10)
clf = DecisionTreeClassifier(random_state=100, max_depth=10, min_samples_leaf=10)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = clf.predict(X_test)


accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


print("confusion_matrix: \n", confusion_matrix(y_test, y_pred))

print(classification_report(y_test, y_pred))

"""# KNN"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset
X = dataset.iloc[:,:-1]
y = dataset.iloc[:,-1]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# KNN classifier
knn = KNeighborsClassifier(n_neighbors=7, p=2, metric='euclidean')
knn.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = knn.predict(X_test_scaled)


accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")


print("confusion_matrix: \n", confusion_matrix(y_test, y_pred))


print(classification_report(y_test, y_pred))

#Plotting to locate the optimum K value of KNN
import math
print("Max K value: ",math.sqrt(len(y_test)))


error_rate = []

# Will take some time
for i in range(1,int(math.sqrt(len(y_test)))):

    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train_scaled,y_train)
    pred_i = knn.predict(X_test_scaled)
    error_rate.append(np.mean(pred_i != y_test))


plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')



"""# Naive Bayes"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()

# Fit only on training data
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

# Apply the same transformation to test data
X_test_scaled = scaler.transform(X_test)

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train.ravel())

g=gnb.predict(X_test)
g[:29]

gnb.predict_proba(X_test)

print("Training accuracy of the model is {:.2f}%".format(gnb.score(X_train, y_train)*100))
print("Testing accuracy of the model is {:.2f}%".format(gnb.score(X_test, y_test)*100))

print("confusion_matrix: \n", confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""
#Logistic regression(Classifier)"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

x = clf.predict([[ 2.29587277,  0.51641122, -0.56669797,  0.45724073, -0.937987  ,
        1.68799354,  0.48438061, -0.20028519,  1.12564857,  0.17806229,
       -0.8630364 ,  0.09665284,  1.63492538,  0.58025552, -0.11094084,
       -1.74809471, -0.75304181, -0.30700794,  1.04749961,  2.19205368,
       -0.92313865]])
if x==[1]:
  print('The water is safe.')
else:
  print('The water is not safe.')

"""# Logistic Regression(after Scaling)

"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scaling
scaler = StandardScaler()


scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)


X_test_scaled = scaler.transform(X_test)

# Logistic Regression
clf = LogisticRegression(max_iter=9000, random_state=0).fit(X_train_scaled, y_train.ravel())

from sklearn.linear_model import LogisticRegression
X = X_train
y = y_train

clf = LogisticRegression(max_iter=9000,random_state=0).fit(X, y.ravel())

c=clf.predict(X_test)
c[:29]

clf.score(X_test,y_test)

clf.predict_proba(X_test)

print("Training accuracy of the model is {:.2f}%".format(clf.score(X_train, y_train)*100))
print("Testing accuracy of the model is {:.2f}%".format(clf.score(X_test, y_test)*100))

print("confusion_matrix: \n", confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# New Section"""

y= dataset_corr.nitrates





"""# **Result analysis of used models**

Model Applied
Accuracy before scaling
                SVM
93.3125
Linear Regression
83.81
Decision Tree
96.93
Naive Bias
89.81
KNN
91
Logistic Regression
83.81
"""